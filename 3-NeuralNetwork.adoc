== 神经网络
关于感知机，既有好消息，也有坏消息。好消息是，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。上一章已经介绍过，即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

=== 从感知机到神经网络
用图来表示神经网络的话，如图 3-1 所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见。

image::images\0616099334542720652325317491870209446299.jpg[title='图3-1 神经网络的例子']

复习感知机，关于输入信号、偏置参数、激活函数，与神经网络进行对比。

=== 激活函数
神经网络中经常使用的一个激活函数就是式（3.6）表示的 sigmoid 函数（sigmoid function）。

image:images/App_2025-07-15_10-07-55.png[]

式（3.6）中的 exp(-x) 表示  的意思。e 是纳皮尔常数 2.7182 ...。

sigmoid 函数的实现:
[source, python]
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

sigmoid 函数和阶跃函数的比较

image:images/061609933454549653813850227315607103054.jpg[title='图 3-8　阶跃函数与 sigmoid 函数（虚线是阶跃函数）']

观察图 3-8，首先注意到的是“平滑性”的不同。sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以 0 为界，输出发生急剧性的变化。sigmoid 函数的平滑性对神经网络的学习具有重要意义。另一个不同点是，相对于阶跃函数只能返回 0 或 1，sigmoid 函数可以返回 0.731 ...、0.880 ... 等实数（这一点和刚才的平滑性有关）。也就是说，感知机中神经元之间流动的是 0 或 1 的二元信号，而神经网络中流动的是连续的实数值信号。

神经网络的激活函数必须使用非线性函数。因为使用线性函数的话，加深神经网络的层数就没有意义了。线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。

 ReLU（Rectified Linear Unit）函数,在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出 0（图 3-9）。

 image:images/0616099334545872644824494563469207905530.jpg[title='图 3-9']

 ReLU 函数可以表示为下面的式 (3.7)。

image:images/App_2025-07-15_10-20-54.png[]

因此，ReLU 函数的实现也很简单，可以写成如下形式。
[source, python]
def relu(x):
    return np.maximum(0, x)

=== 多维数组的运算
用NumPy生成多维数组

[source, terminal]
----
>>> import numpy as np
>>> A = np.array([1, 2, 3, 4])
>>> print(A)
[1 2 3 4]
>>> np.ndim(A) # 数组的维数可以通过 np.ndim() 函数获得
1
>>> A.shape # ，数组的形状可以通过实例变量 shape 获得
(4,)
>>> A.shape[0]
4

>>> B = np.array([[1,2], [3,4], [5,6]])
>>> print(B)
[[1 2] [3 4] [5 6]]
>>> np.ndim(B)
2
>>> B.shape
(3, 2)
----

这里生成了一个 3 × 2 的数组 B。3 × 2 的数组表示第一个维度有 3 个元素，第二个维度有 2 个元素。另外，第一个维度对应第 0 维，第二个维度对应第 1 维（Python 的索引从 0 开始）。二维数组也称为矩阵（matrix）。如图 3-10 所示，数组的横向排列称为行（row），纵向排列称为列
（column）。

image:images/0616099334546242137898475677121733707306.jpg[]

绍矩阵（二维数组）的乘积。比如 2 × 2 的矩阵，其乘积可以像图 3-11 这样进行计算（按图中顺序进行计算是规定好了的）。

image:images/0616099334546466251286924738997018523283.jpg[title='图 3-11　矩阵的乘积的计算方法']

[source, terminal]
>>> A = np.array([[1,2], [3,4]])
>>> A.shape
(2, 2)
>>> B = np.array([[5,6], [7,8]])
>>> B.shape
(2, 2)
>>> np.dot(A, B)
array([[19, 22],       
        [43, 50]])

神经网络的内积

以图 3-14 中的简单神经网络为对象。这个神经网络省略了偏置和激活函数，只有权重。

image:images/0616099334547422586374482406421115800874.jpg[title='图 3-14　通过矩阵的乘积进行神经网络的运算']

[source, terminal]
>>> X = np.array([1, 2])
>>> X.shape
(2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5] [2 4 6]]
>>> W.shape
(2, 3)
>>> Y = np.dot(X, W)
>>> print(Y)
[ 5  11  17]



